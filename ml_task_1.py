# -*- coding: utf-8 -*-
"""ML_Task_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XsKDa14RoKv-ZTvR_X-ID2v-hptn_KhZ

# ðŸ›’ Sales Forecasting Dashboard using Superstore Dataset

#**ðŸ“Œ Problem Statement**

Develop a predictive analytics dashboard that enables retail businesses to anticipate future sales using historical transaction data.

The goal is to provide actionable insights that help businesses:

Manage inventory and stock levels more effectively

Plan marketing campaigns based on demand patterns

Recognize seasonal trends and fluctuations

Forecast sales for the upcoming months

# **ðŸŽ¯ Objectives**

âœ… Forecast future retail sales using historical transaction data

âœ… Visualize:

Actual vs. Predicted Sales

Trend, Seasonal, and Holiday Effects

Sales Breakdown by Product Category, Region, and Month

Identify high-demand periods and potential sales spikes

âœ… Build an interactive Power BI dashboard to present insights and actionable business recommendations

# **ðŸ§° Tools & Technologies**

Python

pandas â†’ Data cleaning & preprocessing

prophet â†’ Time-series forecasting

matplotlib, seaborn â†’ Data visualization

Power BI â†’ Dashboard creation & business insights

The Superstore Sales Dataset contains detailed order-level data, including information on sales, profit, and customer demographics. This data is categorized by product, sub-category, and geographical region.

# ðŸ“¦ **Dataset**

The Superstore Sales Dataset contains detailed order-level data, including information on sales, profit, and customer demographics. This data is categorized by product, sub-category, and geographical region.

#Import the necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""#**Understanding the data**"""

# Load dataset with correct encoding
df = pd.read_csv('/content/drive/MyDrive/Ml tasks/Sample - Superstore.csv' , encoding='ISO-8859-1')
df.head()

"""#**ðŸ§¹Data exploration & quality assessment**"""

df.shape

df.describe()

df.info()

df.duplicated()

"""#**Data cleaning**

To begin our data preparation, we need to curate the dataset by removing any columns that are not useful for our forecasting model. This process involves eliminating fields that are irrelevant to predicting aggregate sales trends, such as internal indices and unique identifiers. The columns we will drop from the dataset are:

Row ID: An internal index.

Order ID: A unique order number.

Customer ID: A unique customer identifier.

Customer Name: The name of the customer.

Postal Code: Specific geographic data that is too granular.

Product ID: A unique product identifier.
"""

columns_to_drop = [
    'Row ID',
    'Order ID',
    'Customer ID',
    'Customer Name',
    'Postal Code',
    'Product ID'
]
df.drop(columns=columns_to_drop, inplace=True)

"""#**Convert date columns & create time- features**

To enhance our time series analysis, we'll transform our date columns into a more useful format. This involves converting the "Order Date" and "Ship Date" fields to a standard datetime format. From the "Order Date," we will then extract several granular features, including the year, month, week, day of the month, and the day of the week. These new features will provide key insights into patterns, helping us to identify trends and seasonal behaviors within the data.
"""

df['Order Date'] = pd.to_datetime(df['Order Date'], format='%m/%d/%Y')
df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%m/%d/%Y')

df['year'] = df['Order Date'].dt.year
df['month'] = df['Order Date'].dt.month
df['week'] = df['Order Date'].dt.isocalendar().week.astype(int)
df['day'] = df['Order Date'].dt.day
df['day_of_week'] = df['Order Date'].dt.dayofweek

df.drop_duplicates(inplace=True)
df.duplicated().sum()

"""To further prepare our dataset, we'll create new features to capture important sales patterns. This includes adding a column for the sales quarter, calculating the monthly average sales, and flagging dates that are likely to have holiday-related sales spikes."""

df['quarter'] = df['Order Date'].dt.quarter

monthly_avg_sales = df.groupby(df['Order Date'].dt.month)['Sales'].mean().reset_index()
# Rename the 'Order Date' column to 'month' in monthly_avg_sales to match the column in df
monthly_avg_sales.rename(columns={'Order Date': 'month', 'Sales': 'monthly_avg_sales'}, inplace=True)

# Drop existing monthly_avg_sales columns before merging to avoid duplicates
df.drop(columns=['monthly_avg_sales_x', 'monthly_avg_sales_y', 'monthly_avg_sales'], errors='ignore', inplace=True)

df = pd.merge(df, monthly_avg_sales, on='month', how='left')

df['is_holiday_spike'] = np.where(
    (df['Order Date'].dt.month == 11) |
    (df['Order Date'].dt.month == 12),
    1, 0
)
df.head()

"""#**Exploratory Data Analysis (EDA)**

We will perform an in-depth exploratory analysis to uncover actionable insights. This involves examining:

Correlations between numerical variables.

Feature Distributions for sales, profit, and quantity.

Performance Metrics across different categories, sub-categories, regions, and segments.

Temporal Trends to identify seasonal and cyclical patterns over time.

These insights will inform our feature engineering and help us select the best forecasting model.
"""

# Correlation heatmap for numerical features
plt.figure(figsize=(10, 7))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap", fontsize=16)
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

sns.histplot(df['Sales'], bins=50, kde=True, ax=axes[0])
axes[0].set_title('Distribution of Sales', fontsize=14)
axes[0].set_xlabel('Sales', fontsize=12)
axes[0].set_ylabel('Frequency', fontsize=12)

sns.histplot(df['Profit'], bins=50, kde=True, ax=axes[1])
axes[1].set_title('Distribution of Profit', fontsize=14)
axes[1].set_xlabel('Profit', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)

plt.tight_layout()
plt.show()

category_summary = df.groupby('Category').agg(
    Total_Sales=('Sales', 'sum'),
    Total_Profit=('Profit', 'sum')
).reset_index().melt(id_vars='Category', var_name='Metric', value_name='Amount')
plt.figure(figsize=(12, 7))
sns.barplot(x='Category', y='Amount', hue='Metric', data=category_summary, palette='tab10')
plt.title('Total Sales vs. Profit by Category', fontsize=16)
plt.xlabel('Category', fontsize=12)
plt.ylabel('Amount', fontsize=12)
plt.tight_layout()
plt.show()

subcategory_summary = df.groupby('Sub-Category').agg(
    Total_Sales=('Sales', 'sum'),
    Total_Profit=('Profit', 'sum')
).reset_index().melt(id_vars='Sub-Category', var_name='Metric', value_name='Amount')

plt.figure(figsize=(18, 10))
sns.barplot(x='Amount', y='Sub-Category', hue='Metric', data=subcategory_summary, palette='tab10')
plt.title('Total Sales vs. Profit by Sub-Category', fontsize=16)
plt.xlabel('Amount', fontsize=12)
plt.ylabel('Sub-Category', fontsize=12)
plt.tight_layout()
plt.show()

region_summary = df.groupby('Region').agg(
    Total_Sales=('Sales', 'sum'),
    Total_Profit=('Profit', 'sum')
).reset_index().melt(id_vars='Region', var_name='Metric', value_name='Amount')

plt.figure(figsize=(10, 6))
sns.barplot(x='Region', y='Amount', hue='Metric', data=region_summary, palette='tab10')
plt.title('Total Sales vs. Profit by Region', fontsize=16)
plt.xlabel('Region', fontsize=12)
plt.ylabel('Amount', fontsize=12)
plt.tight_layout()
plt.show()

segment_summary = df.groupby('Segment').agg(
    Total_Sales=('Sales', 'sum'),
    Total_Profit=('Profit', 'sum')
).reset_index().melt(id_vars='Segment', var_name='Metric', value_name='Amount')

plt.figure(figsize=(10, 6))
sns.barplot(x='Segment', y='Amount', hue='Metric', data=segment_summary, palette='tab10')
plt.title('Total Sales vs. Profit by Segment', fontsize=16)
plt.xlabel('Segment', fontsize=12)
plt.ylabel('Amount', fontsize=12)
plt.tight_layout()
plt.show()

subcategory_profit = df.groupby('Sub-Category')['Profit'].sum().reset_index()

# Get the top 10 most profitable sub-categories
top_10_profit = subcategory_profit.nlargest(10, 'Profit')

# Plot the top 10
plt.figure(figsize=(12, 7))
sns.barplot(x='Profit', y='Sub-Category', data=top_10_profit, palette='GnBu_r')
plt.title('Top 10 Most Profitable Sub-Categories', fontsize=16)
plt.xlabel('Total Profit', fontsize=12)
plt.ylabel('Sub-Category', fontsize=12)
plt.tight_layout()
plt.show()

"""#**Aggregating for Time Series Structuring**

We aggregate total sales by Order Date because time series models like Prophet require:

One row per time point (e.g., per day)

A single target value per date (total daily sales)

This transforms our raw transactional data into a clean daily time series for forecasting.

**Why do we rename columns to ds and y?**

Prophet requires the input DataFrame to have:

ds: the datetime column

y: the numeric target variable

By renaming Order Date â†’ ds and Sales â†’ y, we prepare the dataset for modeling.
"""

daily_sales = (
    df.groupby('Order Date')
      .agg({
          'Sales': 'sum',
          'quarter': 'first',
          'month': 'first',
          'monthly_avg_sales': 'first',
          'is_holiday_spike': 'max'
      })
      .reset_index()
      .rename(columns={'Order Date': 'ds', 'Sales': 'y'})
)

plt.figure(figsize=(14,6))
plt.plot(daily_sales['ds'], daily_sales['y'])
plt.title('Daily Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.show()

"""#**Managing Outliers in profit**

While negative profits are legitimate indicators of losses and will be retained, exceptionally high profit figures have the potential to distort our analysis.

To address this, we will apply a statistical technique. By utilizing the Interquartile Range (IQR) method, we'll establish a ceiling for profit values. Any data point exceeding this upper boundary will be adjusted down to the ceiling, leaving the rest of the dataset's values untouched.
"""

Q1 = df['Profit'].quantile(0.25)
Q3 = df['Profit'].quantile(0.75)
IQR = Q3 - Q1

upper_bound = Q3 + 1.5 * IQR

df['Profit'] = np.where(df['Profit'] > upper_bound, upper_bound, df['Profit'])

"""#**Model Training**"""

from prophet import Prophet


# Keep engineered features
daily_sales['quarter'] = daily_sales['ds'].dt.quarter
daily_sales['month'] = daily_sales['ds'].dt.month

# Bring in monthly_avg_sales (already created earlier)
daily_sales = pd.merge(daily_sales, monthly_avg_sales, on='month', how='left')

# Holiday spike flag
daily_sales['is_holiday_spike'] = np.where(
    (daily_sales['ds'].dt.month == 11) | (daily_sales['ds'].dt.month == 12),
    1, 0
)
holidays = pd.DataFrame({
    'holiday': ['Black Friday']*4 + ['Christmas']*4 + ['New Year']*4,
    'ds': pd.to_datetime([
        '2014-11-28', '2015-11-27', '2016-11-25', '2017-11-24',
        '2014-12-25', '2015-12-25', '2016-12-25', '2017-12-25',
        '2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01'
    ]),
    'lower_window': 0,
    'upper_window': 1
})


model = Prophet(holidays=holidays,
                yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=False)

model.add_seasonality(name='monthly', period=30.5, fourier_order=5)
model.add_regressor('quarter')
model.add_regressor('monthly_avg_sales')
model.add_regressor('is_holiday_spike')

daily_sales['y'] = np.log1p(daily_sales['y'])
model.fit(daily_sales)

future = model.make_future_dataframe(periods=180)
future['quarter'] = future['ds'].dt.quarter
future['month'] = future['ds'].dt.month
future = pd.merge(future, monthly_avg_sales, on='month', how='left')
future['is_holiday_spike'] = np.where(
    (future['ds'].dt.month == 11) | (future['ds'].dt.month == 12),
    1, 0
)

forecast = model.predict(future)
forecast[['yhat','yhat_lower','yhat_upper']] = np.expm1(forecast[['yhat','yhat_lower','yhat_upper']])

daily_sales_plot = daily_sales.copy()
daily_sales_plot['y'] = np.expm1(daily_sales_plot['y'])





plt.figure(figsize=(12,6))

# Actuals (black dots, same as Prophet)
plt.plot(daily_sales_plot['ds'], daily_sales_plot['y'], 'k.', label="Actuals")

# Forecast line (blue, same as Prophet)
plt.plot(forecast['ds'], forecast['yhat'], color='#0072B2', label="Forecast")

# Uncertainty interval (light blue fill, same as Prophet)
plt.fill_between(
    forecast['ds'],
    forecast['yhat_lower'],
    forecast['yhat_upper'],
    color='#0072B2',
    alpha=0.2,
    label="Confidence Interval"
)

plt.title("Forecast of Daily Sales")
plt.xlabel("Date")
plt.ylabel("Sales")
plt.legend()
plt.show()
fig2 = model.plot_components(forecast)
plt.show()

"""#**Model Evaluation**

To evaluate our model's performance, we will use cross-validation. This process involves simulating historical forecasts and measuring how well our model's predictions align with the actual data. The performance will be measured using standard error metrics like:

Mean Absolute Percentage Error (MAPE)

Root Mean Squared Error (RMSE)

Mean Absolute Error (MAE)



These metrics will provide insight into the model's accuracy. A good result would indicate that the model reliably captures the main sales trends, making the forecasts dependable for business decisions.
"""

from prophet.diagnostics import cross_validation, performance_metrics
# horizon: how far into the future to forecast each time (e.g., 90 days)
# initial: size of the initial training period (e.g., 730 days ~ 2 years)
# period: spacing between cutoff dates

df_cv = cross_validation(model,
                         initial='730 days',
                         period='180 days',
                         horizon = '90 days')
df_p = performance_metrics(df_cv)
print(df_p.head())

from prophet.plot import plot_cross_validation_metric

fig = plot_cross_validation_metric(df_cv, metric='mape')
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
daily_sales['y'] = np.expm1(daily_sales['y'])
# Merge forecast with actuals
df_merged = pd.merge(daily_sales, forecast[['ds', 'yhat']], left_on='ds', right_on='ds', how='inner')

# Calculate error metrics
mae = mean_absolute_error(df_merged['y'], df_merged['yhat'])
rmse = np.sqrt(mean_squared_error(df_merged['y'], df_merged['yhat']))
mape = np.mean(np.abs((df_merged['y'] - df_merged['yhat']) / df_merged['y'])) * 100

print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAPE: {mape:.2f}%")

# Save combined dataset for Power BI
df_merged.to_csv('historical_and_forecast_sales.csv', index=False)

"""#**Exporting Dataset**"""

# Get last available date in historical data
last_date = daily_sales['ds'].max()

# Filter only future forecasted dates
future_forecast = forecast[forecast['ds'] > last_date]

# Save future forecast
future_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].to_csv('future_forecasted_sales.csv', index=False)

# Save historical daily sales
daily_sales.to_csv('historical_daily_sales.csv', index=False)

# Save Error matrics
metrics = {
    "Metric": ["MAE", "RMSE", "MAPE"],
    "Value": [mae,rmse, mape]
}
metrics_df = pd.DataFrame(metrics)
metrics_df.to_csv("forecast_metrics.csv", index=False)